learning_rate = 0.02  # ADJUST THIS!
iterations = 250      # AND/OR THIS!

x = torch.tensor(3.0, requires_grad=True)  # Need requires_grad=True
y = torch.tensor(4.0, requires_grad=True)  # Need requires_grad=True

losses = []

for iteration in range(iterations):
    fxy = f(x, y)

    losses.append(fxy.item())
    
    # One step of gradient descent...
    fxy.backward()
    x.data = x.data - learning_rate * x.grad
    y.data = y.data - learning_rate * y.grad
    
    # By default, the gradients will continue to accumulate.
    # We need to zero it out each iteration to get a fresh result.
    x.grad.zero_()
    y.grad.zero_()