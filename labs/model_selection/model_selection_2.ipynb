{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8940fc38b5b67831eadbe8abf38e2a54",
     "grade": false,
     "grade_id": "cell-024f68b84019ea56",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Model Selection and Evaluation - Part 2\n",
    "\n",
    "## Exercise 3 - Cross Validation\n",
    "\n",
    "In the previous activity, we made the classic mistake of tuning our model using our test set. This tends to give misleadingly optimistic predictions about how well we will do on unobserved data. Remember that we carefully picked our hyperparameter values to do as well as possible *on our held-out data*. We shouldn't be surprised when our model performs better on that data than on unobserved data. This problem is particularly acute if our data set is small.\n",
    "\n",
    "The traditional solution is to divide our data into three disjoint sets: **training**, **validation**, and **testing**:\n",
    "* The **training** set is used to fit the model.\n",
    "* The **validation** set is used to evaluate models for the purpose of hyperparameter selection. \n",
    "* The **test** set is kept in a locked room guarded by jaguars. We only look at the testing set ONCE, when we have finalized our model. That way our performance on the test set gives us an unbiased estimate of our generalization error.\n",
    "\n",
    "This traditional approach is fine if we have a lot of data to work with. If the data set is small, we are faced with a painful dilemma: More validation data means better model selection. More testing data means more accurate model evaluation. More training data means better models. Any data we use for one purpose can't be used for the others.\n",
    "\n",
    "**Cross validation** is one way to use limited data more effectively.  The cells below walk us through an example of using cross validation for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to reimport and reload everything...\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sprinkle_data\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Grab our training data\n",
    "source = sprinkle_data.SprinkleDataSource()\n",
    "X, y = source.gen_data(100, seed=2)\n",
    "\n",
    "# Split our data into a training and testing set...\n",
    "split_point = int(X.shape[0] * .8) # Use 80% of the data to train the model\n",
    "\n",
    "X_train = X[:split_point, :]\n",
    "y_train = y[:split_point]\n",
    "\n",
    "X_test = X[split_point:, :] # This data will ONLY be used for final evaluation.\n",
    "y_test = y[split_point:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b877023296368f5410646e81d23c965",
     "grade": false,
     "grade_id": "cell-9b217ef3399e76ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The following cell shows how we can use the scikit-learn `KFold` class to automatically split up our training data for k-fold cross validation. Take a minute to read through this code to make sure you understand what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "folds = 10\n",
    "max_max_leaves = y_train.size\n",
    "kf = KFold(n_splits=folds)\n",
    "\n",
    "mses = np.zeros((folds, max_max_leaves - 2)) # (can't have 0 or 1 leaves)\n",
    "\n",
    "# Loop over all of the hyperparameter settings\n",
    "for max_leaves in range(2, max_max_leaves):\n",
    "    \n",
    "    k = 0\n",
    "    \n",
    "    # Evaluate each one K-times\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_index, :], X_train[val_index, :]\n",
    "        y_tr, y_val = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        tree = DecisionTreeRegressor(max_leaf_nodes=max_leaves)\n",
    "        tree.fit(X_tr, y_tr)\n",
    "        \n",
    "        y_val_predict = tree.predict(X_val)\n",
    "        mses[k, max_leaves - 2] = np.sum((y_val - y_val_predict)**2) / y_val.size\n",
    "        \n",
    "        k += 1\n",
    "        \n",
    "# Average across the k folds\n",
    "mse_avg = np.mean(mses, axis=0)\n",
    "\n",
    "plt.plot(np.arange(2, max_max_leaves), mse_avg, '.-')\n",
    "plt.xlabel('max leaves')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1fcd2b568f17a52cf237475f94bd2f59",
     "grade": false,
     "grade_id": "cell-0e86a6a7f1559582",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "If we are real experts in scikit-learn, we can automate some of this by using the `cross_val_score` function:  \n",
    "\n",
    "(There are also library routines for [automating the entire process of hyperparameter tuning](https://scikit-learn.org/stable/modules/grid_search.html).) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CODE CELL ACCOMPLISHES THE SAME THING AS THE PREVIOUS ONE.\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "mses = np.zeros((folds,max_max_leaves - 2))\n",
    "\n",
    "# Loop over all of the hyperparameter settings\n",
    "for leaves in range(2, max_max_leaves):\n",
    "    tree = DecisionTreeRegressor(max_leaf_nodes=leaves)\n",
    "    \n",
    "    # Returns an array of cross validation results.\n",
    "    mses[:, leaves - 2] = -cross_val_score(tree, X_train, y_train, \n",
    "                                         cv=folds, scoring='neg_mean_squared_error')\n",
    "    \n",
    "mse_avg = np.mean(mses, axis=0)\n",
    "\n",
    "plt.xlabel('max leaves')\n",
    "plt.ylabel('MSE')\n",
    "plt.plot(np.arange(2, 80), mse_avg, '.-')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "95b3ff13d52ca9f9875a8c2c982d3820",
     "grade": false,
     "grade_id": "cell-c42d7ecc970867a7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question\n",
    "\n",
    "* Based on the results above, what is the most promising hyperparameter value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e3e388cca8bb653e9e4354d3420e8ee4",
     "grade": true,
     "grade_id": "ex3_best_leaves",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bef2f5e180053adac9e354d256c3149e",
     "grade": false,
     "grade_id": "cell-ed643140492f15be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that we have a value for our hyperparameter, let's train our final model on the *full* training set and use our locked-away testing set to predict model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeRegressor(max_leaf_nodes=??) # Put your best hyperparameter here!\n",
    "\n",
    "# Train using ALL the training data\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Test on held-out testing data\n",
    "y_test_predict = tree.predict(X_test)\n",
    "mse = np.sum((y_test - y_test_predict)**2) / y_test.size\n",
    "\n",
    "print(\"Predicted MSE: {:.4f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a6ec9af03cece3aa9bf97475657b74b",
     "grade": false,
     "grade_id": "cell-b8560a9fe2023146",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Since none of the data we are testing on here was used *in any way* to design or fit the model, this value should give us an unbiased estimate of our generalization error.  Let's try testing on some new unobserved data to see how good our estimate is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how we do on unobserved data... \n",
    "X_new, y_new = source.gen_data(100000, seed=100)\n",
    "y_new_predict = tree.predict(X_new)\n",
    "mse = np.sum((y_new - y_new_predict)**2) / y_new.size\n",
    "print(\"MSE: {:.4f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f75ac2352416ee6929708438541c2b4",
     "grade": false,
     "grade_id": "ex3_evaluation",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Questions\n",
    "\n",
    "Take a look back at the results from Exercise 2 and answer the following questions:\n",
    "\n",
    "* Did the cross validation approach improve our results in terms of *model selection* (i.e. did we end up with a better model)?  Justify your answer.\n",
    "* Did maintaining a proper test set improve our results in terms of *model evaluation* (i.e. did we make a more accurate prediction about our generalization error)?  Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b7c350fb159e74bea274318e1011413",
     "grade": true,
     "grade_id": "cell-f355595b169010d3",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7180ba9f9fea48f179d41ce814bedbf4",
     "grade": false,
     "grade_id": "cell-209fbbadfa6b8c99",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 4 - Bias Variance Decomposition\n",
    "\n",
    "we spent some time earlier considering the bias/variance decomposition: \n",
    "\n",
    "$$E[(y - \\hat{f}(x))^2] = (Bias[\\hat{f}(x)])^2 + Variance[\\hat{f}(x)] + \\sigma^2$$\n",
    "\n",
    "### Questions\n",
    "\n",
    "* What part of the formula above corresponds to the notion of *generalization error* that we have been discussing?\n",
    "* How is hyperparameter selection related to the Bias term?\n",
    "* How is the training set size related to the Variance term?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6fa6089bcd864d856088640407608c40",
     "grade": true,
     "grade_id": "ex4_bias_variance",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
